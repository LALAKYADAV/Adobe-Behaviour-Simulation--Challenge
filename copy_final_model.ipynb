{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei_465UYKycd",
        "outputId": "a6f8c3b9-569c-4758-824c-5d2b00d70a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZSDDHggK1W3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, BertModel, BertConfig\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1A0lctsehYDs"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJxAjSQaLfJy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"/media/souravsaini/Data/POP_OS/dl/pytorch/pytorch/adobe_mid_prep/behaviour_simulation_train.xlsx\")\n",
        "# df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WAD6Tyq4olwL"
      },
      "source": [
        "## GPU code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n74DDFdColZ5",
        "outputId": "e6cb5443-ab83-432b-fe8f-501d2e14be8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "def shift_device(data):\n",
        "    if torch.cuda.is_available():\n",
        "        return data.to(\"cuda\")\n",
        "    else return data\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "print(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EdDC13EDT0g2"
      },
      "source": [
        "## Tokenizer and Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "bfcf3351c1d94b959947248e29b807b1",
            "67f91f9287584b939520bd76f533bc5d",
            "2ac3dccec6594c1ab9c98b231238b9c5",
            "3b520ad6d28440219f991b9ac67e6d78"
          ]
        },
        "id": "WNf_LN9kNxG7",
        "outputId": "e45a003c-deaa-4482-a28b-33b831cbea04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfcf3351c1d94b959947248e29b807b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67f91f9287584b939520bd76f533bc5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ac3dccec6594c1ab9c98b231238b9c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b520ad6d28440219f991b9ac67e6d78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Hello, this is a <mention> and here is a <hyperlink>.\n",
            "Tokenized text: {'input_ids': tensor([[  101,  7592,  1010,  2023,  2003,  1037, 30523,  1998,  2182,  2003,\n",
            "          1037, 30524,  1012,   102, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522, 30522,\n",
            "         30522, 30522, 30522, 30522, 30522, 30522]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "# Specify your special tokens\n",
        "special_tokens = [\"<mention>\", \"<hyperlink>\"]\n",
        "\n",
        "# Initialize the tokenizer with custom settings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    model_max_length=256,\n",
        "    padding_side='right',\n",
        "    additional_special_tokens=special_tokens,\n",
        "    split_special_tokens=False,\n",
        "    pad_token=\"<pad>\",\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "text = \"Hello, this is a <mention> and here is a <hyperlink>.\"\n",
        "tokens = tokenizer(text, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\n",
        "# tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "print(\"Original text:\", text)\n",
        "print(\"Tokenized text:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHktUUBRUT0U",
        "outputId": "fe6951cf-de20-4d07-ab8b-4966a8fa5280"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30525, 256, padding_idx=0)\n",
              "    (position_embeddings): Embedding(256, 256)\n",
              "    (token_type_embeddings): Embedding(2, 256)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "# Load pre-trained BERT model configuration\n",
        "config = BertConfig(\n",
        "    hidden_size=256,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=8,\n",
        "    max_position_embeddings=256,  # Set the maximum sequence length\n",
        "    vocab_size = 30525,\n",
        ")\n",
        "\n",
        "# Create a custom BERT model\n",
        "model = BertModel(config)\n",
        "to_device(model, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hscO4O1haitT",
        "outputId": "085a7c2c-166d-44f2-d58e-b4044cbc9cc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 256,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 8,\n",
              "  \"num_hidden_layers\": 8,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"transformers_version\": \"4.25.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30525\n",
              "}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2ElRzmc1J46"
      },
      "outputs": [],
      "source": [
        "def get_features_text(text):\n",
        "  new_tokens = tokenizer(text, max_length=256, truncation=True, padding='max_length', return_tensors='pt')\n",
        "  new_tokens = shift_device(new_tokens)\n",
        "  outputs = model(**new_tokens)\n",
        "  return outputs.last_hidden_state"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rReWE94A0awG"
      },
      "source": [
        "## Visuals and audio feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-gbd33438CD"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "def extract_file_extension(url):\n",
        "    try:\n",
        "        # Send a HEAD request to get the Content-Type header\n",
        "        response = requests.head(url)\n",
        "\n",
        "        # Check if Content-Type header is present\n",
        "        if 'content-type' in response.headers:\n",
        "            content_type = response.headers['content-type']\n",
        "\n",
        "            # Extract the file extension from the Content-Type header\n",
        "            file_extension = content_type.split('/')[-1]\n",
        "            return file_extension\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    # If Content-Type is not present or extraction fails, return None\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvvI4oIOmcRQ"
      },
      "outputs": [],
      "source": [
        "class VideoVariant:\n",
        "    def __init__(self, contentType, url, bitrate):\n",
        "        self.contentType = contentType\n",
        "        self.url = url\n",
        "        self.bitrate = bitrate\n",
        "\n",
        "class Video:\n",
        "    def __init__(self, thumbnailUrl, variants, duration, views):\n",
        "        self.thumbnailUrl = thumbnailUrl\n",
        "        self.variants = variants\n",
        "        self.duration = duration\n",
        "        self.views = views\n",
        "\n",
        "class Photo:\n",
        "    def __init__(self, previewUrl, fullUrl):\n",
        "        self.previewUrl = previewUrl\n",
        "        self.fullUrl = fullUrl\n",
        "\n",
        "class Gif:\n",
        "    def __init__(self, thumbnailUrl, variants):\n",
        "        self.thumbnailUrl = thumbnailUrl\n",
        "        self.variants = variants\n",
        "\n",
        "\n",
        "def get_lowest_bitrate_video_or_gif(video_or_gif):\n",
        "    if isinstance(video_or_gif, (Video, Gif)):\n",
        "        variants = getattr(video_or_gif, 'variants', [])\n",
        "        # Filter out variants with bitrate set to None\n",
        "        valid_variants = [v for v in variants if v.bitrate is not None]\n",
        "        if valid_variants:\n",
        "            lowest_bitrate_variant = min(valid_variants, key=lambda v: v.bitrate)\n",
        "            return lowest_bitrate_variant\n",
        "    return None\n",
        "\n",
        "\n",
        "def parse_objects_string(objects_string):\n",
        "    try:\n",
        "        # Using eval to parse the string and create objects\n",
        "        parsed_objects = eval(objects_string)\n",
        "        return parsed_objects\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing objects string: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_lowest_bitrate_video(video):\n",
        "    if isinstance(video, Video):\n",
        "        variants = getattr(video, 'variants', [])\n",
        "        # Filter out variants with bitrate set to None\n",
        "        valid_variants = [v for v in variants if v.bitrate is not None]\n",
        "        if valid_variants:\n",
        "            lowest_bitrate_variant = min(valid_variants, key=lambda v: v.bitrate)\n",
        "            return lowest_bitrate_variant\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U-ZH75Z4BLC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import subprocess\n",
        "from urllib.parse import unquote\n",
        "\n",
        "def download_media(media_info_str, base_download_dir, entry_index):\n",
        "    # store paths\n",
        "    paths = []\n",
        "     # Create a folder for each entry\n",
        "    folder_name = f\"entry_{entry_index}\"\n",
        "    download_dir = os.path.join(base_download_dir, folder_name)\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    # Remove line breaks and parse the objects string\n",
        "    objects_string = media_info_str.replace('\\n', '')\n",
        "    parsed_objects = parse_objects_string(objects_string)\n",
        "\n",
        "    index = 0\n",
        "    # Accessing the attributes of the created objects\n",
        "    for parsed_object in parsed_objects:\n",
        "        media_url = \"\"\n",
        "        if isinstance(parsed_object, (Video, Gif)):\n",
        "            lowest_bitrate_variant = get_lowest_bitrate_video(parsed_object)\n",
        "            if lowest_bitrate_variant:\n",
        "                media_url = lowest_bitrate_variant.url\n",
        "        elif isinstance(parsed_object, Photo):\n",
        "            media_url = parsed_object.fullUrl\n",
        "        if media_url == \"\":\n",
        "          continue\n",
        "        file_name = str(index+1)+\".\"+extract_file_extension(media_url)\n",
        "        subprocess.run(['curl', media_url, '-o', file_name], check=True,cwd = download_dir)\n",
        "        paths.append(download_dir+'/'+file_name)\n",
        "        index += 1\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTW84YMOzfZK"
      },
      "outputs": [],
      "source": [
        "class ModifiedViT(nn.Module):\n",
        "    def __init__(self, original_model, output_size):\n",
        "        super(ModifiedViT, self).__init__()\n",
        "        self.features = original_model\n",
        "        self.head = nn.Linear(original_model.config.num_labels, output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.features(**inputs).logits\n",
        "        x = self.head(x)\n",
        "        x = x.view(-1, 1)  # Reshape the tensor to (256*256, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "5b318b6823f5409bb32e5cdfd7d5f59e",
            "4a14d100e93344d6a51e8c6ac0a4d971",
            "f631c79d700e42039f3059f1072e3385"
          ]
        },
        "id": "m2En3A-j4V0j",
        "outputId": "12c3b3b5-40b0-4702-eea8-b6d1bf2568cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b318b6823f5409bb32e5cdfd7d5f59e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a14d100e93344d6a51e8c6ac0a4d971",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f631c79d700e42039f3059f1072e3385",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import imageio\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define your desired output size and number of frames\n",
        "desired_output_size = 256*256\n",
        "num_frames = 32\n",
        "\n",
        "# Create ViTFeatureExtractor and ViTForImageClassification\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "# to_device(feature_extractor, device)\n",
        "\n",
        "# Modify the model to produce the desired output size\n",
        "original_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "to_device(original_model, device)\n",
        "modified_model = ModifiedViT(original_model, desired_output_size)\n",
        "to_device(modified_model, device)\n",
        "\n",
        "# Function to extract features from a file path\n",
        "def extract_features(file_path, reduction_method=\"pca\"):\n",
        "    # Check if the file is a GIF or video\n",
        "    if file_path.endswith(('.gif', '.apng', '.avif', '.webp')):\n",
        "        gif = imageio.get_reader(file_path)\n",
        "        total_frames = len(gif)\n",
        "        step_size = max(1, total_frames // num_frames)  # Calculate step size\n",
        "        frames = [Image.fromarray(gif.get_data(i)) for i in range(0, total_frames, step_size)]\n",
        "    elif file_path.endswith(('.mp4', '.avi', '.mkv', 'webm', '.mov')):\n",
        "        video = cv2.VideoCapture(file_path)\n",
        "        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        step_size = max(1, total_frames // num_frames)  # Calculate step size\n",
        "        frames = []\n",
        "        for i in range(0, total_frames, step_size):\n",
        "            video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "            ret, frame = video.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(Image.fromarray(frame))\n",
        "        video.release()\n",
        "    else:\n",
        "        # Assume it's an image file\n",
        "        frames = [Image.open(file_path)]\n",
        "\n",
        "    # Extract features from each frame\n",
        "    features_list = []\n",
        "    for frame in frames:\n",
        "        # Preprocess the image\n",
        "        frame = frame.convert(\"RGB\")  # Ensure RGB format\n",
        "        inputs = feature_extractor(images=frame, return_tensors=\"pt\", to=\"pt\")\n",
        "        inputs = shift_device(inputs)\n",
        "        output_vector = modified_model(inputs)\n",
        "        output_vector = output_vector\n",
        "        features_list.append(output_vector.flatten().detach())  # Flatten to (256*256,) and detach gradients\n",
        "\n",
        "    # Stack to get (256*256, num_frames)\n",
        "    features_array = torch.stack(features_list, axis=1)\n",
        "\n",
        "    # Reduction by PCA, or mean (NOTE: Significantly slow on mp4 files)\n",
        "    if(reduction_method==\"pca\"):\n",
        "        pca = PCA(n_components=1)\n",
        "        feature_vector = pca.fit_transform(features_array)\n",
        "\n",
        "        return feature_vector\n",
        "    else:\n",
        "        feature_vector = np.mean(features_array, axis=1, keepdims=True)\n",
        "\n",
        "        return feature_vector\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eI008Gzqzyby"
      },
      "source": [
        "Tester (Do NOT run)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VXcpHYiIEgWQ"
      },
      "source": [
        "## combining the textual features and visual features using cross attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rfraycGFDsv",
        "outputId": "800c2294-fac0-4f65-c76d-1e3ee8978cb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FeatureCombiningModel(\n",
              "  (cross_attention): CrossAttention(\n",
              "    (linear_q): Linear(in_features=256, out_features=2048, bias=True)\n",
              "    (linear_kv): Linear(in_features=256, out_features=4096, bias=True)\n",
              "    (linear_out): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (multihead_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Linear transformation for queries\n",
        "        self.linear_q = nn.Linear(hidden_dim, hidden_dim * num_heads)\n",
        "\n",
        "        # Linear transformation for keys and values\n",
        "        self.linear_kv = nn.Linear(hidden_dim, hidden_dim * num_heads * 2)\n",
        "\n",
        "        # Linear transformation for the output of attention\n",
        "        self.linear_out = nn.Linear(hidden_dim * num_heads, hidden_dim)\n",
        "\n",
        "        # Multihead attention\n",
        "        self.multihead_attn = nn.MultiheadAttention(hidden_dim, num_heads)\n",
        "\n",
        "    def forward(self, text_features, media_features):\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        q = self.linear_q(text_features)\n",
        "        kv = self.linear_kv(media_features)\n",
        "\n",
        "        # Split linear_kv output into keys and values\n",
        "        k, v = torch.split(kv, split_size_or_sections=self.hidden_dim * self.num_heads, dim=-1)\n",
        "\n",
        "        # Reshape for multihead attention\n",
        "        q = q.view(q.size(0), -1, self.hidden_dim)\n",
        "        k = k.view(k.size(0), -1, self.hidden_dim)\n",
        "        v = v.view(v.size(0), -1, self.hidden_dim)\n",
        "\n",
        "        # Multihead attention\n",
        "        attn_output, _ = self.multihead_attn(q, k, v)\n",
        "\n",
        "        # Reshape and apply linear transformation\n",
        "        attn_output = attn_output.view(attn_output.size(0), -1)\n",
        "        output = self.linear_out(attn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class FeatureCombiningModel(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super(FeatureCombiningModel, self).__init__()\n",
        "\n",
        "        # Cross-Attention module\n",
        "        self.cross_attention = CrossAttention(hidden_dim, num_heads)\n",
        "        to_device(self.cross_attention, device)\n",
        "\n",
        "    def forward(self, text_features, media_features):\n",
        "        # Apply cross-attention\n",
        "        cross_attn_output = self.cross_attention(text_features, media_features)\n",
        "\n",
        "        return cross_attn_output\n",
        "\n",
        "\n",
        "# Instantiate the custom model\n",
        "CombineFeature = FeatureCombiningModel(hidden_dim=256, num_heads=8)\n",
        "to_device(CombineFeature, device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OPknahyt6CKx"
      },
      "source": [
        "Loading as .npy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5_IEie6qZQB"
      },
      "outputs": [],
      "source": [
        "# # for each entry, take \"full\" portion, process, and write back as \".npy\" file\n",
        "# base_download_dir = 'downloaded_media'\n",
        "# base_path = '/media/souravsaini/Data/POP_OS/dl/pytorch/pytorch/adobe_mid_prep/Dataset/JonnySet'\n",
        "# for index, row in df.iterrows():\n",
        "#     media_info_str = row['media']\n",
        "#     paths = download_media(media_info_str, base_download_dir, index + 1)\n",
        "#     image_feature = np.zeros(shape=(256*256,1))\n",
        "#     try:\n",
        "#       for path in paths:\n",
        "#         # doubt:  condition for image to be processed\n",
        "#         image_feature  +=  extract_features(path,\"\")\n",
        "#       text_info_str = row[\"content\"]\n",
        "#       text_features = get_features_text(text_info_str)\n",
        "#       image_feature = torch.from_numpy(image_feature)\n",
        "#       image_feature = image_feature.to(torch.float32)\n",
        "#       text_features = text_features.to(torch.float32).reshape(256,256)\n",
        "#       features = CombineFeature(image_feature.reshape(256, 256), text_features.reshape(256,256))\n",
        "#       as_numpy = features.detach().numpy()\n",
        "#       torch.save(as_numpy,os.path.join(base_path,f'{index+1}.npy'))\n",
        "#     except:\n",
        "#       print(\"skipped:\"+str(index+1))\n",
        "#     print(index+1)\n",
        "#     # save as numpy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0BdLDCYC5snU"
      },
      "source": [
        "Do not run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ36m4Of3X6G"
      },
      "outputs": [],
      "source": [
        "# from joblib import Parallel, delayed\n",
        "\n",
        "# features_list = []\n",
        "# labels = []\n",
        "# def process(index, row):\n",
        "#   media_info_str = row['media']\n",
        "#   paths = download_media(media_info_str, base_download_dir, index + 1)\n",
        "#   image_feature = np.zeros(shape=(256*256,1))\n",
        "#   try:\n",
        "#     for path in paths:\n",
        "#       # doubt:  condition for image to be processed\n",
        "#       image_feature  +=  extract_features(path, \"\")\n",
        "#     text_info_str = row[\"content\"]\n",
        "#     text_features = get_features_text(text_info_str)\n",
        "#     image_feature = torch.from_numpy(image_feature)\n",
        "#     image_feature = image_feature.to(torch.float32).to(\"cuda\")\n",
        "#     text_features = text_features.to(torch.float32).reshape(256,256).to(\"cuda\")\n",
        "#     features = CombineFeature(image_feature.reshape(256, 256), text_features.reshape(256,256))\n",
        "#     # as_numpy = features.detach().cpu()\n",
        "#     features_list.append(features.detach().cpu())\n",
        "#     labels.append(torch.tensor(row['likes']))\n",
        "#     # torch.save(as_numpy,os.path.join(base_path,f'{index+1}.npy'))\n",
        "#   except:\n",
        "#     print(\"skipped:\"+str(index+1))\n",
        "#   print(index+1)\n",
        "#   return 0\n",
        "#   # return features_list, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9J2wVZh32ma"
      },
      "outputs": [],
      "source": [
        "# results = Parallel(n_jobs=2)(delayed(process)(i, r) for i, r in df.iterrows())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G5Lp9bLZtJCm",
        "outputId": "35fe87d3-dcef-43c7-d76f-1dc7c02367ae"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (1833612486.py, line 9)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn [17], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    if index = 30000:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
          ]
        }
      ],
      "source": [
        "# # for each entry, take \"full\" portion, process, and write back as \".npy\" file\n",
        "# # from joblib import Parallel, delayed\n",
        "\n",
        "# base_download_dir = 'downloaded_media'\n",
        "# base_path = '/media/souravsaini/Data/POP_OS/dl/pytorch/pytorch/adobe_mid_prep/Dataset/JonnySet'\n",
        "# features_list = []\n",
        "# labels = []\n",
        "# for index, row in df.iterrows():\n",
        "#     if index = 30000:\n",
        "#       break\n",
        "#     media_info_str = row['media']\n",
        "#     paths = download_media(media_info_str, base_download_dir, index + 1)\n",
        "#     image_feature = np.zeros(shape=(256*256,1))\n",
        "#     try:\n",
        "#       for path in paths:\n",
        "#         # doubt:  condition for image to be processed\n",
        "#         image_feature  +=  extract_features(path, \"\")\n",
        "#       text_info_str = row[\"content\"]\n",
        "#       text_features = get_features_text(text_info_str)\n",
        "#       image_feature = torch.from_numpy(image_feature)\n",
        "#       image_feature = image_feature.to(torch.float32).to(\"cuda\")\n",
        "#       text_features = text_features.to(torch.float32).reshape(256,256).to(\"cuda\")\n",
        "#       features = CombineFeature(image_feature.reshape(256, 256), text_features.reshape(256,256))\n",
        "#       # as_numpy = features.detach().cpu()\n",
        "#       features_list.append(features.detach().cpu())\n",
        "#       labels.append(torch.tensor(index))\n",
        "#       # torch.save(as_numpy,os.path.join(base_path,f'{index+1}.npy'))\n",
        "#     except:\n",
        "#       print(\"skipped:\"+str(index+1))\n",
        "#     print(index+1)\n",
        "\n",
        "# stacked_features = torch.stack(features_list, dim=0)\n",
        "# labels = torch.stack(labels, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paWf3b7-3BEu"
      },
      "outputs": [],
      "source": [
        "base_download_dir = 'downloaded_media'\n",
        "base_path = '/content/drive/MyDrive/mid_prep/dataset'\n",
        "strt_idx = 96852   # change here\n",
        "diff = 3150\n",
        "for i in range(strt_idx, strt_idx+diff+1): #[)\n",
        "  media_info_str = df.iat[i, 5]\n",
        "  paths = download_media(media_info_str, base_download_dir, i + 1)\n",
        "  image_list = []\n",
        "  try:\n",
        "    for path in paths:\n",
        "    # doubt:  condition for image to be processed\n",
        "    image_list.append(extract_features(path, \"\"))\n",
        "    image_feature = torch.stack(image_list, dim=0)\n",
        "\n",
        "    # image_feature = torch.from_numpy(image_feature)\n",
        "    image_feature = image_feature.to(torch.float32).reshape(-1,256,256).detach()\n",
        "    # as_numpy = features.detach().cpu()\n",
        "    np.save(base_path + \"/\" + i + \".pt\", image_feature)\n",
        "#   labels.append(i)\n",
        "# torch.save(as_numpy,os.path.join(base_path,f'{index+1}.npy'))\n",
        "  except:\n",
        "    print(\"skipped:\"+str(i+1))\n",
        "  print(i+1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqECoAOz0InE"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xcJyJgNvFikH"
      },
      "source": [
        "Tester code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9yOuAhXyLix"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
